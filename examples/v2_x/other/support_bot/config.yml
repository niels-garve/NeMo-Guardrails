colang_version: "2.x"

# streaming: True

models:
  - type: main
    engine: nim
    model: llama3.1
    parameters:
      base_url: http://0.0.0.0:4000/chat/completions
      # stream: true

# Collection of all the prompts taken from `nemoguardrails/llm/prompts/llama3.yml`
prompts:
  # Colang 2 prompts below.

  # Prompt for detecting the user message canonical form.
  - task: generate_user_intent_from_user_action
    models:
      - llama3
      - llama3.1
    messages:
      - type: system
        content: |-
          Derive `user intent:` from user action considering the intents from section 'These are the most likely user
          intents:'. Always start your answer with "user intent: " and then your choice of intent:
          ```text
          user intent: [your choice]
          ```

      - type: system
        content: |-
          "These are the most likely user intents:"
          {{ examples }}

      - type: user
        content: "user action: {{ user_action }}"

      - type: system
        content: "Your answer:"

    output_parser: "my_user_intent"

  - task: generate_user_intent_and_bot_action_from_user_action
    models:
      - llama3
      - llama3.1
    messages:
      - type: system
        content: "{{ general_instructions }}"

      - type: system
        content: "This is how a conversation between a user and the bot can go:"
      - "{{ sample_conversation | to_messages_v2 }}"

      - type: system
        content: |-
          "These are the most likely user intents:"
          {{ examples }}

      - type: system
        content: "This is the current conversation between the user and the bot:"
      - "{{ history | colang | to_messages_v2}}"

      - type: user
        content: "user action: {{ user_action }}"

      - type: system
        content: "Continuation of the interaction starting with a `user intent:` from the section 'These are the most likely user intents':"

  # Prompt for generating the value of a context variable.
  - task: generate_value_from_instruction
    models:
      - llama3
      - llama3.1
    messages:
      - type: system
        content: |
          {{ general_instructions }}

          Your task is to generate value for the ${{ var_name }} variable..
          Do not provide any explanations, just output value.

      - type: system
        content: "This is how a conversation between a user and the bot can go:"
      - "{{ sample_conversation | to_messages_v2 }}"

      - type: system
        content: "This is the current conversation between the user and the bot:"
      - "{{ history | colang | to_messages_v2}}"

      - type: assistant
        content: |
          Follow these instruction `{{ instructions }}` to generate a value that is assigned to:
          ${{ var_name }} =

  # Prompt for generating a flow from instructions.
  - task: generate_flow_from_instructions
    models:
      - llama3
      - llama3.1
    content: |-
      # Example flows:
      {{ examples }}

      # Complete the following flow based on its instruction:
      flow {{ flow_name }}
        """{{ instructions }}"""

  # Prompt for generating a flow from name.
  - task: generate_flow_from_name
    models:
      - llama3
      - llama3.1
    messages:
      - type: system
        content: |
          {{ general_instructions }}

          Your task is to generate a flow from the provided flow name ${{ flow_name }}.
          Do not provide any explanations, just output value.

      - type: system
        content: "This is the current conversation between the user and the bot:"
      - "{{ history | colang | to_messages_v2}}"

      - type: system
        content: |-
          These are some example flows:
          {{ examples }}

      - type: system
        content: |-
          Complete the following flow based on its name:
          flow {{ flow_name }}

          Do not provide any explanations, just output value.
    stop:
      - "\nflow"

    # Prompt for generating the continuation for the current conversation.
  - task: generate_flow_continuation
    models:
      - llama3
      - llama3.1
    messages:
      - type: system
        content: "{{ general_instructions }}"

      - type: system
        content: "This is how a conversation between a user and the bot can go:"
      - "{{ sample_conversation | to_messages_v2 }}"

      - type: system
        content: "This is the current conversation between the user and the bot:"
      - "{{ history | colang | to_messages_v2 }}"

      - type: system
        content: "Continuation of interaction:"

  - task: generate_flow_continuation_from_flow_nld
    models:
      - llama3
      - llama3.1
    messages:
      - type: system
        content: "Directly response with expected answer. Don't provide any pre- or post-explanations."
      - type: user
        content: |-
          {{ flow_nld }}
